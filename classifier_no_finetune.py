# -*- coding: utf-8 -*-
"""classifier_no_finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lkjTdIuKwtHJLjWT9rjhnqrlzU9NntqD
"""

# ===================================================================
# Step 1: Environment Setup & Library Installation
# ===================================================================
# Install required libraries:
# - sentence-transformers: to load pre-trained sentence embedding models
# - pyarrow: for reading parquet files
# - pandas, scikit-learn, matplotlib: standard data analysis & ML stack
!pip install -U sentence-transformers scikit-learn pandas pyarrow matplotlib phonenumbers tldextract
!pip install fuzzywuzzy python-levenshtein

import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import phonenumbers
import tldextract
import json
import re # Used for cleaning the input string before parsing
from sklearn.ensemble import GradientBoostingClassifier

import torch

from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint as sp_randint # For integer distributions
from scipy.stats import uniform as sp_uniform # For float distributions


# Scikit-learn imports for modeling and evaluation
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, precision_recall_curve, auc, classification_report, roc_auc_score

# SentenceTransformers imports
from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim

# Optional: Check if a GPU is available in Colab
import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Device in use: {device.upper()}")

"""Define Helper Functions"""

def safe_json_parse(text, key='primary'):
    """
    Safely parse a JSON string and extract a specified key.

    Typical usage in this dataset:
    - Extract 'primary' from names/categories
    - Extract 'freeform' from addresses
    """
    if text is None:
        return ""
    try:
        data = json.loads(text)
        if isinstance(data, dict):
            return data.get(key, "")
        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
            # Many address records are stored as a list of dicts with 'freeform' fields
            return data[0].get('freeform', "")
        return ""
    except (json.JSONDecodeError, TypeError):
        return ""

def normalize_phone(text, country_code):
    """
    Extract and normalize phone numbers to E.164 format (+CCNNNNNNNNN) using the provided country code.

    Parameters
    ----------
    text: str
        List of phone numbers (usually a single list)

    country code: str
        Country code of the input phone number (ex. "GB")

    Returns
    -------
    set : A normalized phone number in E.164 format format.
    """
    if not text or not country_code:
        return ""
    try:
        data = json.loads(text)
        phone_str = None

        # Extract the raw phone string from the list structure (similar to previous logic)
        if isinstance(data, list) and len(data) > 0:
            first_element = data[0]
            if isinstance(first_element, str):
                phone_str = first_element
            elif isinstance(first_element, dict):
                phone_str = first_element.get('value', first_element.get('phone'))

            if phone_str and isinstance(phone_str, str):
                # 1. Attempt to parse using the known country_code
                parsed_num = phonenumbers.parse(phone_str, country_code)

                # 2. If valid, format it to the unique E.164 string
                if phonenumbers.is_valid_number(parsed_num):
                    return phonenumbers.format_number(parsed_num, phonenumbers.PhoneNumberFormat.E164)

        return ""
    except (json.JSONDecodeError, TypeError, phonenumbers.NumberParseException):
        # Catch any data parsing or number parsing errors
        return ""

# Assuming you've already defined safe_json_parse, add this simple extraction function:
def extract_country_code(text):
    """Safely extracts the two-letter country code from the address JSON."""
    if not text:
        return ""
    try:
        data = json.loads(text)
        if isinstance(data, list) and len(data) > 0 and isinstance(data[0], dict):
            return data[0].get('country', "")
        return ""
    except (json.JSONDecodeError, TypeError):
        return ""

def get_base_domain(text_list):
    """
    Extract and normalize websites

    Parameters
    ----------
    text_list : str
        List of websites as a string

    Returns
    -------
    set : A set holding the first website in the list
    """
    if not text_list:
        return ""

    try:
        data = json.loads(text_list)

        if isinstance(data, list) and len(data) > 0:
            url_string = None
            first_element = data[0]

            url_string = first_element

            if url_string and isinstance(url_string, str):
                # Use tldextract to get the domain and TLD (ignoring subdomains)
                extracted = tldextract.extract(url_string)

                # Check if we got a valid domain back
                if extracted.domain and extracted.suffix:
                    return f"{extracted.domain}.{extracted.suffix}"

        return ""
    except (json.JSONDecodeError, TypeError):
        return ""

STREET_CLASSIFIERS = {
    'street', 'st', 'avenue', 'ave', 'road', 'rd', 'boulevard', 'blvd',
    'drive', 'dr', 'court', 'ct', 'ln', 'lane', 'pl', 'place', 'circle', 'cir',
    'highway', 'hwy', 'expressway', 'fwy', 'terrace', 'ter', 'sq', 'square',
    'n', 's', 'e', 'w', 'ne', 'nw', 'se', 'sw'
}

def create_text_representation(row, column, key="freeform", base=False):
    """
    Create a clean, unified text description for a place record.

    Parameters
    ----------
    row : pd.Series
        One row from the dataset.
    base : bool
        If True, use the 'base_' columns (the reference place).
        If False, use the candidate columns.

    Returns
    -------
    str : A single concatenated string containing name, category, and address.
    """
    prefix = 'base_' if base else ''

    text = safe_json_parse(row[f'{prefix}{column}'], key=key)

    if column == "addresses":
        text = text.lower().strip()

        # 2. Remove common punctuation and symbols (e.g., periods, commas)
        text = re.sub(r'[.,#\-]', '', text)

    if not text:
      text = ""

    # Concatenate non-empty fields into one text string
    parts = [part for part in [text] if part]
    return ", ".join(parts)

def normalize_social_identifier(text_list):
    """
    Extract and normalize socials

    Parameters
    ----------
    text_list : str
        List of socials as a string

    Returns
    -------
    set : A set of all normalized socials
    """
    identifiers = set()
    if not text_list:
        return identifiers

    try:
        data = json.loads(text_list)

        if isinstance(data, list):
            for id_string in data:

                if id_string:
                    # Basic normalization: lowercase, strip whitespace, strip protocol/www
                    normalized_id = id_string.lower().strip().strip('/').replace('https://', '').replace('http://', '').lstrip('www.')
                    if normalized_id:
                        identifiers.add(normalized_id)

    except (json.JSONDecodeError, TypeError):
        pass

    return identifiers

import json

def get_all_categories(text):
    """
    Safely parses the category JSON and returns a set containing the primary
    category and all alternate categories (lowercased and cleaned).
    """
    categories = set()
    if not text:
        return categories
    try:
        data = json.loads(text)

        if isinstance(data, dict):
            # 1. Add Primary Category
            primary = data.get('primary')
            if primary:
                categories.add(primary.lower().strip())

            # 2. Add Alternate Categories
            alternates = data.get('alternate')
            if isinstance(alternates, list):
                for alt in alternates:
                    if isinstance(alt, str):
                        categories.add(alt.lower().strip())

        return categories

    except (json.JSONDecodeError, TypeError):
        return categories

def extract_and_clean_number(address_text: str) -> str:
    """
    Safely extracts the primary numerical identifier (usually the street number)
    from a cleaned address string. Returns only digits and letters that precede
    the first main space, or the first number found.
    """
    if not isinstance(address_text, str) or not address_text:
        return ""

    # Simple strategy: find the first sequence of numbers/letters before the first space
    # Example: "12720 4th ave w" -> "12720"
    # Example: "Ground Floor, Kalyani Chambers, Plot No 80, Nigdi" -> "Ground" (Need a better parser)

    # A more robust strategy for US/Western addresses: find the initial number sequence.
    match = re.match(r'^[\d\s\w]+', address_text.lower().strip())
    if match:
        # Extract the whole starting segment (e.g., "12720 4th")
        starting_segment = match.group(0).split()

        # Take the first token that contains digits, which is usually the house number
        for token in starting_segment:
            if re.search(r'\d', token):
                # Clean the token: remove trailing letters often seen in addresses (e.g., 'A', 'Bldg C')
                # This ensures '123A' matches '123' if the letter is noise.
                return re.sub(r'[a-z]+$', '', token.strip('.,-'))

    return ""

"""Data Loading and Cleaning"""

# ===================================================================
# Step 2: Data Loading & Cleaning
# ===================================================================
# Assume the dataset file has already been uploaded to Colab
file_path = "samples_3k_project_c_updated.parquet"
try:
    df = pd.read_parquet(file_path)
    print(f"Dataset loaded successfully. Shape: {df.shape}")
except FileNotFoundError:
    print(f"Error: File '{file_path}' not found. Please upload it before proceeding.")
    exit()

# Apply the function to create two new columns
tqdm.pandas(desc="Creating text representations")
df['name'] = df.progress_apply(lambda row: create_text_representation(row, "names", key="primary", base=False), axis=1)
df['base_name'] = df.progress_apply(lambda row: create_text_representation(row, "names", key="primary", base=True), axis=1)

df['address'] = df.progress_apply(lambda row: create_text_representation(row, "addresses", key="freeform", base=False), axis=1)
df['base_address'] = df.progress_apply(lambda row: create_text_representation(row, "addresses", key="freeform", base=True), axis=1)

df['category'] = df.progress_apply(lambda row: create_text_representation(row, "categories", key="primary", base=False), axis=1)
df['base_category'] = df.progress_apply(lambda row: create_text_representation(row, "categories", key="primary", base=True), axis=1)

print("\nData cleaning & text representation complete.")

# Compare normalized phone numbers
df['country_code'] = df['base_addresses'].progress_apply(extract_country_code)

df['candidate_phone_norm'] = df.progress_apply(
    lambda row: normalize_phone(row['phones'], row['country_code']),
    axis=1
)
df['base_phone_norm'] = df.progress_apply(
    lambda row: normalize_phone(row['base_phones'], row['country_code']),
    axis=1
)

df['phone_match'] = (
    (df['candidate_phone_norm'] != "") &
    (df['base_phone_norm'] != "") &
    (df['candidate_phone_norm'] == df['base_phone_norm'])
).astype(int)

# Create confidence columns
df['candidate_confidence_score'] = df['confidence']
df['base_confidence_score'] = df['base_confidence']

df['confidence_product'] = df['confidence'] * df['base_confidence']

# Compare socials for binary match feature
# df['candidate_social_ids'] = df['socials'].apply(normalize_social_identifier)
# df['base_social_ids'] = df['base_socials'].apply(normalize_social_identifier)

"""
df['social_match'] = df.apply(
    lambda row: 1
    if (row['candidate_social_ids'].intersection(row['base_social_ids'])) # No overlap
    else 0,
    axis=1
)
"""
# Compare websites for binary match feature
df['website'] = df['websites'].apply(get_base_domain)
df['base_website'] = df['base_websites'].apply(get_base_domain)
df['website_match'] = (
    (df['base_website'] != "") &
    (df['website'] != "") &
    (df['website'] == df['base_website'])
).astype(int)

# Match any categories
df['category_set'] = df['categories'].apply(get_all_categories)
df['base_category_set'] = df['base_categories'].apply(get_all_categories)

df['category_match'] = df.apply(
    lambda row: 1 if row['category_set'].intersection(row['base_category_set']) else 0,
    axis=1
)

# Get the differences in length of the names of the two places
df['name_length_diff'] = np.abs(
    df['name'].str.len() - df['base_name'].str.len()
)

from rapidfuzz import fuzz

def calculate_fuzzy_features(name1: str, name2: str) -> dict:
    """
    Calculates key fuzzy ratios between two strings, normalized to 0.0 to 1.0.
    """
    # Ensure inputs are clean (lowercase and stripped of leading/trailing space)
    str1 = str(name1).lower().strip()
    str2 = str(name2).lower().strip()

    # Check if either string is empty to prevent errors or false high scores
    if not str1 or not str2:
        return {
            "ratio": 0.0,
            "partial_ratio": 0.0,
            "token_set_ratio": 0.0
        }

    return {
        # 1. Standard Similarity: Measures overall character similarity
        "ratio": fuzz.ratio(str1, str2) / 100.0,

        # 2. Partial Similarity: Finds the best matching substring (good for long vs. short)
        "partial_ratio": fuzz.partial_ratio(str1, str2) / 100.0,

        # 3. Token Set Ratio: Ignores word order and redundant/extra words (BEST for record linkage)
        "token_set_ratio": fuzz.token_set_ratio(str1, str2) / 100.0,
    }

# Apply the function to create a DataFrame of the scores
fuzzy_scores_df = df.apply(
    lambda row: pd.Series(
        calculate_fuzzy_features(row['name'], row['base_name'])
    ),
    axis=1
)

df = pd.concat([df, fuzzy_scores_df], axis=1)

# Apply the function to create a DataFrame of the scores
fuzzy_scores_addr_df = df.apply(
    lambda row: pd.Series(
        calculate_fuzzy_features(row['address'], row['base_address'])
    ),
    axis=1
)

fuzzy_scores_addr_df = fuzzy_scores_addr_df.rename(columns={
    'ratio': 'addr_ratio',
    'partial_ratio': 'addr_partial_ratio',
    'token_set_ratio': 'addr_token_set_ratio'
})

df = pd.concat([df, fuzzy_scores_addr_df], axis=1)

df['candidate_addr_num'] = df['address'].apply(extract_and_clean_number)
df['base_addr_num'] = df['base_address'].apply(extract_and_clean_number)

# 2. Create the Binary Mismatch Veto Feature
df['address_number_mismatch'] = df.apply(
    lambda row: 1
    if (row['candidate_addr_num']) and  # Both must have extracted a number
       (row['base_addr_num']) and
       (row['candidate_addr_num'] != row['base_addr_num']) # The numbers must be different
    else 0,
    axis=1
)

df['address_exact_match'] = df.apply(
    lambda row: 1
    if (row['address']) and  # Both must have extracted a number
       (row['base_address']) and
       (row['address'] != row['base_address']) # The numbers must be different
    else 0,
    axis=1
)

"""Split data, Load Models, and Train Classifier"""

# ===================================================================
# Step 3: Create Proxy Labels & Split Data (Avoiding Leakage)
# ===================================================================

# print(f"\nLabel distribution:\n{df['label'].value_counts()}")
# print(f"Positive sample (match) ratio: {df['label'].mean():.2%}")

# --- Critical: Prevent data leakage ---
# We split the dataset BEFORE generating embeddings.
# Otherwise, test data might influence the embedding space.
# Use embedding, phone numbers, and website/socials as features
"""X = df[['name', 'base_name', 'address', 'base_address', 'phone_match',
        'category_match', 'confidence_product',
        'name_length_diff', 'website_match', 'partial_ratio', 'token_set_ratio', 'category',
        'base_category', 'address_number_mismatch', 'addr_partial_ratio', 'addr_token_set_ratio',
        'address_exact_match']]"""
# y = df['label']

from io import BytesIO
from IPython.display import FileLink, display
import os
from typing import Tuple, List, Dict

def load_split_sets() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """
    Loads the saved training and testing Parquet files and restores them
    to the X_train, X_test, y_train, and y_test variables.

    Assumes the files 'conflation_train_set.parquet' and 'conflation_test_set.parquet'
    are available in the current environment.
    """

    TRAIN_FILE = 'conflation_train_set.parquet'
    TEST_FILE = 'conflation_test_set.parquet'
    LABEL_COLUMN = 'true_label'

    try:
        # Load Training Data
        df_train = pd.read_parquet(TRAIN_FILE)
        X_train = df_train.drop(columns=[LABEL_COLUMN])
        y_train = df_train[LABEL_COLUMN]

        # Load Testing Data
        df_test = pd.read_parquet(TEST_FILE)
        X_test = df_test.drop(columns=[LABEL_COLUMN])
        y_test = df_test[LABEL_COLUMN]

        print(f"Successfully loaded datasets.")
        print(f"X_train size: {len(X_train)}")
        print(f"X_test size: {len(X_test)}")

        return X_train, X_test, y_train, y_test

    except FileNotFoundError as e:
        print(f"Error: Could not find required file. Please ensure '{e.filename}' is uploaded.")
        raise

# --- Example Execution ---
X_train, X_test, y_train, y_test = load_split_sets()

# ===================================================================
# Step 4: Load Model & Generate Semantic Embeddings
# ===================================================================
print("\nLoading Sentence Transformer model...")
# Load onto GPU (if available) or CPU
model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
# model_name = 'Qwen/Qwen3-Embedding-0.6B'
model = SentenceTransformer(model_name, device=device)
# model_qwen = model
model_qwen = SentenceTransformer('Qwen/Qwen3-Embedding-0.6B', device=device)
print("Model loaded successfully.")

# Encode both candidate and base text for train and test sets
print("\nEncoding training set...")
# --- Assuming model (MiniLM) and model_qwen (Qwen) are defined and loaded ---

def process_semantic_features(X_set: pd.DataFrame, set_name: str, embedder_map: Dict) -> Tuple[Dict, List[np.ndarray]]:
    """
    Encodes specified text features using designated models, computes cosine similarity,
    and returns both the raw embeddings and the similarity scores.
    """
    print(f"\nEncoding {set_name} set...")

    # Define which columns use which model
    feature_configs = {
        'name': embedder_map['name'],
        'address': embedder_map['address'],
        'category': embedder_map['category'],
    }

    # Store all generated arrays
    embeddings = {}
    similarity_scores = []

    for feature_key, embedder in feature_configs.items():
        cand_col = feature_key
        base_col = f'base_{feature_key}'

        # 1. Generate Embeddings
        # Uses the assigned model (e.g., model_qwen for 'name')
        cand_emb = embedder.encode(X_set[cand_col].tolist(), convert_to_tensor=True, show_progress_bar=True)
        base_emb = embedder.encode(X_set[base_col].tolist(), convert_to_tensor=True, show_progress_bar=True)

        # Store raw embeddings (useful if you want element-wise features later)
        embeddings[f'{set_name}_{feature_key}_emb'] = cand_emb
        embeddings[f'{set_name}_base_{feature_key}_emb'] = base_emb

        # 2. Compute Cosine Similarity
        sim_scores = cos_sim(cand_emb, base_emb).diag().cpu().numpy()
        similarity_scores.append(sim_scores)

    return embeddings, similarity_scores

# --- Application in Pipeline (Replaces all of Step 4 and the similarity part of Step 5) ---

embedder_map = {
    'name': model_qwen, 'address': model, 'category': model_qwen,
}

# Process HARD Training Data
all_train_embeddings, train_similarity_list = process_semantic_features(X_train, 'train', embedder_map)

# Process HARD Testing Data
all_test_embeddings, test_similarity_list = process_semantic_features(X_test, 'test', embedder_map)

# The variables for Step 5 are now:
# train_similarity_list = [Name Sim, Address Sim, Category Sim] (1D NumPy arrays)
# test_similarity_list = [Name Sim, Address Sim, Category Sim] (1D NumPy arrays)

def reshape_similarities(sims):
  reshaped_sims = []
  for sim in sims:
      reshaped_sims.append(sim.reshape(-1, 1))
  return reshaped_sims

reshaped_train_sim = reshape_similarities(train_similarity_list)
reshaped_test_sim = reshape_similarities(test_similarity_list)

# 1. Prepare Semantic Similarity Feature (Feature 1)
# Reshape from 1D array (list of scores) to 2D column array

# --- Assuming all necessary embedding and discrete features are available in X_train, X_test, etc. ---

# Define the list of all discrete and fuzzy columns (10 features)
DISCRETE_COLUMNS = [
    'phone_match', 'confidence_product',
    'category_match', 'name_length_diff', 'website_match',
    'token_set_ratio', 'partial_ratio',
    'address_number_mismatch', 'addr_partial_ratio', 'addr_token_set_ratio'
]

# --- 1. FEATURE STACKING (Using Full X_train and X_test) ---

def reshape_similarities(sims):
    # This function is correct for reshaping the semantic scores
    return [sim.reshape(-1, 1) for sim in sims]

reshaped_train_sim = reshape_similarities(train_similarity_list)
reshaped_test_sim = reshape_similarities(test_similarity_list)

def reshape_and_stack_features(X_data, sim_list, discrete_cols):
    """Reshapes and stacks all semantic and discrete features from the full dataset."""

    # 1. Get the discrete features from the full X_data set
    discrete_features_list = [X_data[col].values.reshape(-1, 1) for col in discrete_cols]

    # 2. Combine the reshaped discrete features with the reshaped similarity scores
    return np.hstack(discrete_features_list + sim_list)


# Create the final input matrix (Full Training Set)
X_train_final = reshape_and_stack_features(X_train, reshaped_train_sim, DISCRETE_COLUMNS)
X_test_final = reshape_and_stack_features(X_test, reshaped_test_sim, DISCRETE_COLUMNS)

# The full labels are y_train and y_test

# ===================================================================
# Step 5: Train Classifier (On ALL Training Data)
# ===================================================================

X_CV = X_train_final
y_CV = y_train

# Final Classifier Definition using Hyperparameters found by Optuna
classifier = GradientBoostingClassifier(
    n_estimators=396,
    learning_rate=0.01277786941047816,
    max_depth=6,
    subsample=0.7240084014559961,
    max_features=0.7,
    min_samples_leaf=7,
    random_state=42
)

# 2. Define the Cross-Validation Strategy (e.g., 5-Fold)
cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 3. Perform Cross-Validation
print("\n" + "="*60)
print(f"Running 5-Fold Stratified Cross-Validation on Gradient Boosting (Full Train Set)")
print("="*60)

cv_f1_scores = cross_val_score(
    classifier, X_CV, y_CV, cv=cv_strategy, scoring='f1', n_jobs=-1
)

print(f"\nCross-Validation F1 Scores (per fold): {cv_f1_scores}")
print(f"Average F1 Score: {cv_f1_scores.mean():.4f}")
print(f"Standard Deviation: Â± {cv_f1_scores.std():.4f}")

# Fit the classifier on the entire training set
classifier.fit(X_train_final, y_train)
print("Classifier training complete (on full training set).")


# ===================================================================
# Step 6: Model Evaluation (Full Test Set)
# ===================================================================
print("\nEvaluating on the test set...")
# Predict probabilities for PR-AUC
y_pred_proba = classifier.predict_proba(X_test_final)[:, 1]
# Predict hard labels for F1
y_pred = classifier.predict(X_test_final)

# 1. Compute F1-score
f1 = f1_score(y_test, y_pred)
print(f"\nF1-Score: {f1:.4f}")

"""Compute Scores"""

file_path = "samples_3k_project_c_updated.parquet"
try:
    df = pd.read_parquet(file_path)
    print(f"Dataset loaded successfully. Shape: {df.shape}")
except FileNotFoundError:
    print(f"Error: File '{file_path}' not found. Please upload it before proceeding.")
    exit()

# 1. Compute F1-score
f1 = f1_score(y_test, y_pred)
print(f"\nF1-Score: {f1:.4f}")

# 2. Compute PR-AUC
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
pr_auc = auc(recall, precision)
print(f"PR-AUC: {pr_auc:.4f}")

# 3. Detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# 4. Plot Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='b', label=f'PR curve (AUC = {pr_auc:.4f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc='best')
plt.grid(True)
plt.show()


"""
Without Fine Tuning
F1-Score: 0.9147
PR-AUC: 0.9705

Classification Report:
              precision    recall  f1-score   support

         0.0       0.89      0.84      0.86       238
         1.0       0.90      0.93      0.91       362

    accuracy                           0.90       600
   macro avg       0.89      0.88      0.89       600
weighted avg       0.89      0.90      0.89       600
"""

# 1. Get the indices of the original rows that were used for the test set
test_indices = X_test.index

# 2. Extract the true labels (y_test) and hard predictions (y_pred)
# Note: y_test is a Pandas Series, y_pred is a NumPy array.
df_results = pd.DataFrame({
    'True_Label': y_test,
    'Prediction': y_pred
}, index=test_indices)

# 3. Add the original records (X_test) and model probabilities (y_pred_proba)
df_results['Prob_Match'] = y_pred_proba
df_results = df_results.join(X_test) # Join with the original features used for testing

# Assuming 'df_labeled' is the DataFrame after Step 2/3 filtering
# and that 'id' and 'base_id' are available columns in df_labeled.

# Filter for False Positives (FP)
# Prediction = 1 AND True_Label = 0
df_FP = df_results[
    (df_results['Prediction'] == 1) & (df_results['True_Label'] == 0)
].sort_values(by='Prob_Match', ascending=False).copy()

# --- MODIFICATION: ADD ID COLUMNS ---
# Get the original IDs based on the index of the FP rows
fp_ids = df.loc[df_FP.index, ['id', 'base_id']]
df_FP = df_FP.join(fp_ids)

print("\n--- FALSE POSITIVES (FP: Wrong Merges) ---")
print(f"Total FP count: {len(df_FP)}")
# Include 'id' and 'base_id' in the display list
print(df_FP[['Prob_Match', 'id', 'base_id', 'name', 'base_name', 'address', 'base_address']].head(10).to_string())


# Filter for False Negatives (FN)
# Prediction = 0 AND True_Label = 1
df_FN = df_results[
    (df_results['Prediction'] == 0) & (df_results['True_Label'] == 1)
].sort_values(by='Prob_Match', ascending=True).copy()

# --- MODIFICATION: ADD ID COLUMNS ---
# Get the original IDs based on the index of the FN rows
fn_ids = df.loc[df_FN.index, ['id', 'base_id']]
df_FN = df_FN.join(fn_ids)

print("\n--- FALSE NEGATIVES (FN: Missed Duplicates) ---")
print(f"Total FN count: {len(df_FN)}")
# Include 'id' and 'base_id' in the display list
print(df_FN[['Prob_Match', 'id', 'base_id', 'name', 'base_name', 'address', 'base_address']].head(10).to_string())

"""!pip install optuna
import optuna
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import GradientBoostingClassifier
import numpy as np

# X_CV and y_CV must be defined globally before running this code
# We assume X_CV is X_train_final and y_CV is y_train from your pipeline

def objective(trial):
    """
    Defines the training process and returns the score to be maximized (F1-Score).
    """
    # 1. Define the Search Space (Optuna suggests parameters for the current trial)
    params = {
        # n_estimators: Range for the number of trees
        'n_estimators': trial.suggest_int('n_estimators', 150, 400),

        # learning_rate: Range for the step size (lower is often better)
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),

        # max_depth: Range for tree complexity (keep it shallow)
        'max_depth': trial.suggest_int('max_depth', 3, 6),

        # subsample: Fraction of samples for each tree (mitigates overfitting)
        'subsample': trial.suggest_float('subsample', 0.6, 0.95),

        # max_features: Number of features to consider for best split
        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5, 0.7]),

        # min_samples_leaf: Regularization parameter
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 15)
    }

    # 2. Define the Model and CV Strategy
    model = GradientBoostingClassifier(**params, random_state=42)
    cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # 3. Perform Cross-Validation
    # We use n_jobs=-1 for parallel processing across the 5 folds
    cv_f1_scores = cross_val_score(
        model,
        X_CV,
        y_CV,
        cv=cv_strategy,
        scoring='f1',
        n_jobs=-1
    )

    # 4. Return the mean score for Optuna to maximize
    return cv_f1_scores.mean()

# --- Execution Block ---

# 1. Create a study object (stores the optimization history)
# direction='maximize' tells Optuna to look for the highest possible F1-Score
study = optuna.create_study(direction='maximize')

print("\nStarting Bayesian Hyperparameter Optimization (Optuna)...")

# 2. Run the optimization for a set number of trials
# Increase n_trials (e.g., to 100 or 200) if you have more time/resources.
study.optimize(objective, n_trials=50)

# 3. Print Final Results
print("\n" + "="*60)
print("Bayesian Optimization Complete")
print("="*60)
print(f"Best Average F1 Score: {study.best_value:.4f}")
print("Best Hyperparameters Found:")
print(study.best_params)

# --- Final Step: Initialize the Final Model with Best Parameters ---
best_classifier = GradientBoostingClassifier(**study.best_params, random_state=42)"""

"""
# Assuming X_train, X_test, y_train, and y_test are defined from the train_test_split

def save_and_download_split_sets(X_train, y_train, X_test, y_test):
    """
    Combines features and labels for both train and test sets, saves them
    as compressed Parquet files, and provides download links.
    """
    print("\nPreparing segregated training and testing sets...")

    # --- 1. Combine and Save Training Set ---
    df_train_full = X_train.copy()
    df_train_full['true_label'] = y_train.values
    train_filename = 'conflation_train_set.parquet'
    df_train_full.to_parquet(train_filename, index=True, compression='snappy')
    print(f"Training set saved as: {train_filename}")

    # --- 2. Combine and Save Testing Set ---
    df_test_full = X_test.copy()
    df_test_full['true_label'] = y_test.values
    test_filename = 'conflation_test_set.parquet'
    df_test_full.to_parquet(test_filename, index=True, compression='snappy')
    print(f"Testing set saved as: {test_filename}")

    # --- 3. Generate Download Links ---
    print("\n--- Download Links ---")

    # Display the file size (optional, but good for context)
    print(f"Train File Size: {os.path.getsize(train_filename) / (1024**2):.2f} MB")
    print(f"Test File Size: {os.path.getsize(test_filename) / (1024**2):.2f} MB")

    # Generate clickable links
    display(FileLink(train_filename))
    display(FileLink(test_filename))


# --- Example Execution ---
# Assumes the train_test_split block just ran
# save_and_download_split_sets(X_train, y_train, X_test, y_test)"""